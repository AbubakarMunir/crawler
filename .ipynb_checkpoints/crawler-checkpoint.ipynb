{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "\n",
    "This notebook contains started code structure for creating a crawler on single machine\n",
    "\n",
    "**Author:** Noshaba Nasir\n",
    "\n",
    "**Date:** 26/3/2021\n",
    "\n",
    "**Updated by:** 17L4545\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import urllib.parse\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import bs4\n",
    "import _thread\n",
    "import urllib.robotparser as urobot\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urljoin\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Parameters\n",
    "BACKQUEUES= 3\n",
    "THREADS= BACKQUEUES*3\n",
    "FRONTQUEUES= 5\n",
    "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
    "\n",
    "# Add any other global parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRONTIER\n",
    "\n",
    "Frontier should use the Mercator frontier design as discussed in lecture.\n",
    "\n",
    "Preferably it should be a class and should have the given functions.\n",
    "\n",
    "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class frontier:\n",
    "# add the code for frontier here\n",
    "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "    def __init__(self,urls):\n",
    "        self.frontqueues = [[] for _ in range(FRONTQUEUES)]\n",
    "        self.backqueues = [[] for _ in range(BACKQUEUES)]\n",
    "        self.add_URLs(urls)\n",
    "        i = 0\n",
    "        while i <=8:\n",
    "            self.add_to_backqueue()\n",
    "            i=i+1\n",
    "        self.timestamps=[]\n",
    "        None # replace this with your code\n",
    "        \n",
    "    # add more functions here\n",
    "    \n",
    "    def prioritizer(self,URL,f):\n",
    "        return random.randint(0,f-1)\n",
    "    \n",
    "    def add_URLs(self,urls):\n",
    "        for url in urls:\n",
    "            priority=self.prioritizer(urls,FRONTQUEUES)\n",
    "            #print(priority)\n",
    "            self.frontqueues[priority].append(url)\n",
    "            \n",
    "            \n",
    "    def add_to_backqueue(self):\n",
    "        \n",
    "        #extracts a url from front queues on base of priority probability\n",
    "        url=''\n",
    "        qno=0    #to store no of front q from which url is taken\n",
    "        prob=random.randint(1,14)\n",
    "        if(prob<5 and len(self.frontqueues[0])!=0):\n",
    "            url=self.frontqueues[0].pop(0)  #pop from frontq 0\n",
    "            qno=0\n",
    "        elif(prob<9 and len(self.frontqueues[1])!=0):\n",
    "            url=self.frontqueues[1].pop(0)  #pop from frontq 0\n",
    "            qno=1\n",
    "        elif(prob<12 and len(self.frontqueues[2])!=0):\n",
    "            url=self.frontqueues[2].pop(0)  #pop from frontq 0\n",
    "            qno=2\n",
    "        elif(prob<14 and len(self.frontqueues[3])!=0):\n",
    "            url=self.frontqueues[3].pop(0)  #pop from frontq 0\n",
    "            qno=3\n",
    "        else:\n",
    "            if(len(self.frontqueues[4])!=0):\n",
    "                url=self.frontqueues[4].pop(0)  #pop from frontq 0\n",
    "                qno=4\n",
    "       \n",
    "        #parsing the hostname of url extratcted from frontqueues\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        host=parsed_url.netloc\n",
    "        \n",
    "        #checks if a backqueue exists with same URL host and adds provided url to that bqueue\n",
    "        for bq in self.backqueues:\n",
    "            if(len(bq)!=0):\n",
    "                parsedurl = urllib.parse.urlparse(bq[0])\n",
    "                domain=parsedurl.netloc\n",
    "                if(domain==host):\n",
    "                    bq.append(url)\n",
    "                    return\n",
    "        #checks if an empty bq exists after iterating through all bqs and checking their hosts\n",
    "        for bq in self.backqueues:\n",
    "            if len(bq) == 0:\n",
    "                bq.append(url)\n",
    "                return\n",
    "        self.frontqueues[qno].append(url)  #add to frontqueue if not added to backqueue\n",
    "        \n",
    "\n",
    "     \n",
    "    def get_URL(self):\n",
    "        url=''\n",
    "        if(len(self.timestamps)==0):\n",
    "            i=0\n",
    "            while i <BACKQUEUES:\n",
    "                self.timestamps.append(datetime.datetime.utcnow())\n",
    "                time.sleep(1)\n",
    "                i=i+1\n",
    "        \n",
    "        min_index = self.timestamps.index(min(self.timestamps))\n",
    "        if(self.timestamps[min_index]< datetime.datetime.utcnow()):\n",
    "            if(len(self.backqueues[min_index])!=0):\n",
    "                url=self.backqueues[min_index].pop(0)\n",
    "                return url\n",
    "                added_seconds = datetime.timedelta(0, 15)\n",
    "                self.timestamps[min_index]=datetime.datetime.utcnow()+added_seconds\n",
    "            return url\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "        \n",
    "#Helper functions\n",
    "\n",
    "#function to return all urls embedded on a html page\n",
    "def fetch(url):\n",
    "    resp = requests.get(url)\n",
    "    with open('file.xml', 'wb') as foutput:\n",
    "        foutput.write(resp.content)\n",
    "    foutput.close()\n",
    "\n",
    "    links_arr=[]\n",
    "    parser = 'html.parser'  \n",
    "    resp = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        lnk  = link.get('href')\n",
    "        if not lnk.startswith('http'):\n",
    "            lnk = urljoin(url, lnk)\n",
    "        #if '#' in lnk or doctype(lnk)==False :\n",
    "            #continue\n",
    "        if '#' in lnk:\n",
    "            continue\n",
    "        else:    \n",
    "            links_arr.append(lnk)\n",
    "\n",
    "    return(links_arr)\n",
    "\n",
    "#returns true if its link of a doctype=html \n",
    "def doctype(url):\n",
    "    try:\n",
    "        parser = 'html.parser'  \n",
    "        resp = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "        items = [item for item in soup.contents if isinstance(item, bs4.Doctype)]\n",
    "        if len(items)!=0:\n",
    "            if items[0]=='html':\n",
    "                return True\n",
    "        return false\n",
    "    except:\n",
    "        return False\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER URLS\n",
    "\n",
    "Filter the URLS that are in robots.txt files of server and the have been already processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering function returns true if link is allowed by robots.txt of respective site\n",
    "def check_if_allowed(url):\n",
    "    url = \"https://docs.oracle.com/cd/E52090_12/\"\n",
    "    parsedurl = urllib.parse.urlparse(url)\n",
    "    domain=parsedurl.netloc\n",
    "  \n",
    "    rp = urobot.RobotFileParser()\n",
    "    rp.set_url(\"https://\" +domain + \"/robots.txt\")\n",
    "    rp.read()\n",
    "    if rp.can_fetch(\"*\", url):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------Write any other codes/data require to run the crawler above this cell-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theard task\n",
    "def crawler_thread_task(threadname):\n",
    "    lnk=frontier_obj.get_URL()\n",
    "    print(lnk)\n",
    "    try:\n",
    "        arr=fetch(lnk)\n",
    "        for lnks in arr:\n",
    "            if check_if_allowed(lnks)==False:\n",
    "                links=[]\n",
    "                links.append(lnks)\n",
    "                frontier_obj.add_URLs(links)\n",
    "                print(lnks)\n",
    "            \n",
    "            frontier_obj.add_to_backqueue()\n",
    "        print(frontier_obj.frontqueues)\n",
    "        print(threadname)\n",
    "       \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "        print(threadname+\"pass\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait for me first before starting the thread\n"
     ]
    }
   ],
   "source": [
    "# intialize every thing \n",
    "listurls=['https://docs.oracle.com/en/','https://www.oracle.com/corporate/','https://en.wikipedia.org/wiki/Machine_learning','https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html','https://docs.oracle.com/middleware/jet210/jet/index.html','https://en.wikipedia.org/w/api.php','https://en.wikipedia.org/api/','https://en.wikipedia.org/wiki/Weka_(machine_learning)']\n",
    "frontier_obj=frontier(listurls)\n",
    "print(\"wait for me first before starting the thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "https://docs.oracle.com/en/\n",
      "thread1pass\n",
      "\n",
      "thread2pass\n",
      "[['https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html', 'https://docs.oracle.com/middleware/jet210/jet/index.html', 'https://en.wikipedia.org/w/api.php', 'https://www.oracle.com/corporate/'], [], [], [], []]\n",
      "thread3\n"
     ]
    }
   ],
   "source": [
    "# start the threads\n",
    "\n",
    "try:\n",
    "   _thread.start_new_thread(crawler_thread_task,(\"thread1\",))\n",
    "   _thread.start_new_thread(crawler_thread_task,(\"thread2\",))\n",
    "   _thread.start_new_thread(crawler_thread_task,(\"thread3\",))\n",
    "    \n",
    "    \n",
    "except:\n",
    "   print(\"Error: unable to start thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: unable to start thread\n"
     ]
    }
   ],
   "source": [
    "# start the threads in loop not working\n",
    "\n",
    "try:\n",
    "    name=\"thread\"\n",
    "    while i<THREADS:\n",
    "        _thread.start_new_thread(crawler_thread_task,(name+i,))\n",
    "        i=i+1\n",
    "     \n",
    "except:\n",
    "   print(\"Error: unable to start thread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------End of Notebook---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
